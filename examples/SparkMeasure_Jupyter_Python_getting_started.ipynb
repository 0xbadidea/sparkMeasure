{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook to showcase sparkMeasure APIs for Python\n",
    "  \n",
    "References:  \n",
    "- [https://github.com/LucaCanali/sparkMeasure](https://github.com/LucaCanali/sparkMeasure)  \n",
    "- sparkmeasure Python docs: [docs/Python_shell_and_Jupyter](https://github.com/LucaCanali/sparkMeasure/blob/master/docs/Python_shell_and_Jupyter.md)  \n",
    "\n",
    "Author: Luca.Canali@cern.ch  \n",
    "July 2018  \n",
    "Last updated August 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies:  \n",
    "    - install sparkmeasure.py Python wrapper package if not already done:  \n",
    "    `pip install sparkmeasure`  \n",
    "    - install spark:  \n",
    "    ` pip install pyspark`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Spark Session\n",
    "# This uses local mode for simplicity\n",
    "# the use of findspark is optional\n",
    "\n",
    "# import findspark\n",
    "# findspark.init(\"/home/luca/Spark/spark-3.3.0-bin-hadoop3\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Test sparkmeasure instrumentation of Python/PySpark code\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.12:0.18\")\n",
    "         .getOrCreate()\n",
    "        )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Python API for sparkmeasure package\n",
    "# and attach the sparkMeasure Listener for stagemetrics to the active Spark session\n",
    "\n",
    "from sparkmeasure import StageMetrics\n",
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cell and line magic to wrap the instrumentation\n",
    "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
    "\n",
    "@register_line_cell_magic\n",
    "def sparkmeasure(line, cell=None):\n",
    "    \"run and measure spark workload. Use: %sparkmeasure or %%sparkmeasure\"\n",
    "    val = cell if cell is not None else line\n",
    "    stagemetrics.begin()\n",
    "    eval(val)\n",
    "    stagemetrics.end()\n",
    "    stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 17\n",
      "elapsedTime => 1261 (1 s)\n",
      "stageDuration => 1031 (1 s)\n",
      "executorRunTime => 2816 (3 s)\n",
      "executorCpuTime => 2248 (2 s)\n",
      "executorDeserializeTime => 2630 (3 s)\n",
      "executorDeserializeCpuTime => 979 (1.0 s)\n",
      "resultSerializationTime => 10 (10 ms)\n",
      "jvmGCTime => 72 (72 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 13 (13 ms)\n",
      "resultSize => 16134 (15.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 0 duration => 555 (0.6 s)\n",
      "Stage 1 duration => 409 (0.4 s)\n",
      "Stage 3 duration => 67 (67 ms)\n"
     ]
    }
   ],
   "source": [
    "%%sparkmeasure\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 17\n",
      "elapsedTime => 461 (0.5 s)\n",
      "stageDuration => 401 (0.4 s)\n",
      "executorRunTime => 2273 (2 s)\n",
      "executorCpuTime => 1909 (2 s)\n",
      "executorDeserializeTime => 63 (63 ms)\n",
      "executorDeserializeCpuTime => 38 (38 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 0 (0 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 4 (4 ms)\n",
      "resultSize => 16048 (15.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 4 duration => 20 (20 ms)\n",
      "Stage 5 duration => 364 (0.4 s)\n",
      "Stage 7 duration => 17 (17 ms)\n"
     ]
    }
   ],
   "source": [
    "# You can also explicitly Wrap your Spark workload into stagemetrics instrumentation \n",
    "# as in this example\n",
    "stagemetrics.begin()\n",
    "\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()\n",
    "\n",
    "stagemetrics.end()\n",
    "# Print a summary report\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 3\n",
      "numTasks => 17\n",
      "elapsedTime => 510 (0.5 s)\n",
      "stageDuration => 399 (0.4 s)\n",
      "executorRunTime => 2255 (2 s)\n",
      "executorCpuTime => 1930 (2 s)\n",
      "executorDeserializeTime => 75 (75 ms)\n",
      "executorDeserializeCpuTime => 43 (43 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 0 (0 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 37 (37 ms)\n",
      "resultSize => 16048 (15.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n",
      "\n",
      "Stages and their duration:\n",
      "Stage 8 duration => 23 (23 ms)\n",
      "Stage 9 duration => 360 (0.4 s)\n",
      "Stage 11 duration => 16 (16 ms)\n"
     ]
    }
   ],
   "source": [
    "# Another way to encapsulate code and instrumentation in a compact form\n",
    "\n",
    "stagemetrics.runandmeasure(locals(), \"\"\"\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of collecting using Task Metrics\n",
    "Collecting Spark task metrics at the granularity of each task completion has additional overhead\n",
    "compare to collecting at the stage completion level, therefore this option should only be used if you need data with this finer granularity, for example because you want\n",
    "to study skew effects, otherwise consider using stagemetrics aggregation as preferred choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  count(1)|\n",
      "+----------+\n",
      "|1000000000|\n",
      "+----------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 8\n",
      "\n",
      "Aggregated Spark task metrics:\n",
      "numTasks => 17\n",
      "successful tasks => 17\n",
      "speculative tasks => 0\n",
      "taskDuration => 2310 (2 s)\n",
      "schedulerDelayTime => 85 (85 ms)\n",
      "executorRunTime => 2168 (2 s)\n",
      "executorCpuTime => 1952 (2 s)\n",
      "executorDeserializeTime => 57 (57 ms)\n",
      "executorDeserializeCpuTime => 28 (28 ms)\n",
      "resultSerializationTime => 0 (0 ms)\n",
      "jvmGCTime => 0 (0 ms)\n",
      "shuffleFetchWaitTime => 0 (0 ms)\n",
      "shuffleWriteTime => 0 (0 ms)\n",
      "gettingResultTime => 0 (0 ms)\n",
      "resultSize => 2667 (2.0 KB)\n",
      "diskBytesSpilled => 0 (0 Bytes)\n",
      "memoryBytesSpilled => 0 (0 Bytes)\n",
      "peakExecutionMemory => 0\n",
      "recordsRead => 2000\n",
      "bytesRead => 0 (0 Bytes)\n",
      "recordsWritten => 0\n",
      "bytesWritten => 0 (0 Bytes)\n",
      "shuffleRecordsRead => 8\n",
      "shuffleTotalBlocksFetched => 8\n",
      "shuffleLocalBlocksFetched => 8\n",
      "shuffleRemoteBlocksFetched => 0\n",
      "shuffleTotalBytesRead => 472 (472 Bytes)\n",
      "shuffleLocalBytesRead => 472 (472 Bytes)\n",
      "shuffleRemoteBytesRead => 0 (0 Bytes)\n",
      "shuffleRemoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffleBytesWritten => 472 (472 Bytes)\n",
      "shuffleRecordsWritten => 8\n"
     ]
    }
   ],
   "source": [
    "from sparkmeasure import TaskMetrics\n",
    "taskmetrics = TaskMetrics(spark)\n",
    "\n",
    "taskmetrics.begin()\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(1000)\").show()\n",
    "taskmetrics.end()\n",
    "taskmetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
